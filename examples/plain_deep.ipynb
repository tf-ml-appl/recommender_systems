{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"eTRqE7QUoYRm"},"outputs":[],"source":["# pip install tensorflow-datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qVocM4Jqpg0d"},"outputs":[],"source":["from collections import defaultdict\n","from collections import namedtuple\n","\n","import tensorflow_datasets as tfds\n","import tensorflow as tf\n","import numpy as np\n","\n","import logging\n","import math"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XmjdDrqc6UIl"},"outputs":[],"source":["logging.getLogger().setLevel(logging.INFO)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PytEcWdxenDl"},"outputs":[],"source":["BATCH_SIZE = 128*16\n","OUTPUT_UNITS = 5\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2d7iB0n4-3-l"},"outputs":[],"source":["EmbeddingParams = namedtuple(\n","    'EmbeddingParams', [\n","        'input_dim',\n","        'output_dim',\n","        'embeddings_initializer',\n","        'embeddings_regularizer',\n","        'activity_regularizer',\n","        'embeddings_constraint',\n","        'mask_zero',\n","        'input_length',\n","        'sparse', # not used, not recognized in tensorflow 2.11.0\n","        'kwargs'],\n","        defaults=[10, 32, 'uniform', None, None, None, False, None, False, {}]\n",")\n","\n","Conv1dParams = namedtuple(\n","    'Conv1dParams', [\n","        'filters', \n","        'kernel_size', \n","        'strides', \n","        'padding', \n","        'data_format', \n","        'dilation_rate',\n","        'groups', \n","        'activation', \n","        'use_bias', \n","        'kernel_initializer', \n","        'bias_initializer',\n","        'kernel_regularizer',\n","        'bias_regularizer',\n","        'activity_regularizer', \n","        'kernel_constraint',\n","        'bias_constraint',\n","        'kwargs'],\n","        defaults=[1, 5, 1, 'valid', 'channels_last', 1, 1, None, True, 'glorot_uniform', 'zeros', None, None, None, None, None, {}])\n","\n","DenseParams = namedtuple(\n","    'DenseParams', [\n","        'units', \n","        'activation', \n","        'use_bias', \n","        'kernel_initializer',\n","        'bias_initializer', \n","        'kernel_regularizer',\n","        'bias_regularizer',\n","        'activity_regularizer', \n","        'kernel_constraint', \n","        'bias_constraint',\n","        'kwargs'],\n","        defaults=[10, None, True, 'glorot_uniform', 'zeros', None, None, None, None, None, {}])\n","\n","BatchnormParams = namedtuple(\n","    'BatchnormParams', [\n","        'axis', \n","        'momentum', \n","        'epsilon', \n","        'center', \n","        'scale', \n","        'beta_initializer', \n","        'gamma_initializer', \n","        'moving_mean_initializer', \n","        'moving_variance_initializer',\n","        'beta_regularizer',\n","        'gamma_regularizer', \n","        'beta_constraint', \n","        'gamma_constraint',\n","        'synchronized', # not used, cannot found in tf-2.11.0\n","        'kwargs'],\n","        defaults=[1, 0.99, 0.001, True, True, 'zeros', 'ones', 'zeros', 'ones', None, None, None, None, False, {}])                                                          \n","\n","DropoutParams = namedtuple(\n","    'DropoutParams', [\n","        'rate', \n","        'noise_shape', \n","        'seed',\n","        'kwargs'],\n","        defaults=[0.8, None, None, {}])\n","\n","ModelParams = namedtuple(\n","    'ModelParams', [\n","        'num_embedding_layers',\n","        'embedding_params',\n","        'num_dense_layers',\n","        'conv1d_params',\n","        'dense_params',\n","        'batchnorm_params',\n","        'dropout_params',\n","        'final_dense_params'\n","    ]\n",")\n","# assumes conv1d and dense layers are interleaved, thus # layers are different.\n","# may explore other interleaving architecture.\n","def constructModelParams(num_embedding_layers=1,\n","                         embedding_input_dims=[10],\n","                         embedding_output_dims=[32],\n","                         output_units=1,\n","                         conv1d_filters=8,\n","                         conv1d_filter_dim=11,\n","                         dense_shrink_multiplier=4):\n","  if (num_embedding_layers!=len(embedding_input_dims)) or (\n","      num_embedding_layers!=len(embedding_output_dims)):\n","      raise ValueError(f'length of embedding vocab_sizes and output dims must be equal to' +\n","                       f' num_embedding_layers: {num_embedding_layers} vs ' +\n","                       f'{len(embedding_input_dims)} vs {len(embedding_output_dims)}.')\n","  estimated_conv1d_output = np.sum(embedding_output_dims)*conv1d_filters\n","  num_dense_layers = int(math.log(estimated_conv1d_output/(output_units*dense_shrink_multiplier), dense_shrink_multiplier))\n","  logging.info(f'number of dense layers: {num_dense_layers}')\n","  embedding_params = []\n","  for i in range(num_embedding_layers):\n","    embedding_params.append(\n","        EmbeddingParams(embedding_input_dims[i], embedding_output_dims[i]))\n","  conv1d_params = []\n","  for i in range(num_dense_layers):\n","    conv1d_params.append(Conv1dParams(conv1d_filters, conv1d_filter_dim))\n","    conv1d_filter_dim -= 2\n","    conv1d_filter_dim = max(1, conv1d_filter_dim)\n","\n","  dense_params = []\n","  dense_dim = estimated_conv1d_output\n","  for i in range(num_dense_layers):\n","    dense_dim = int(dense_dim/dense_shrink_multiplier)\n","    dense_dim = max(1, dense_dim)\n","    dense_params.append(DenseParams(dense_dim, 'relu'))\n","    logging.info(f'estimated {i}th dense layer dim: {dense_dim}')\n","\n","  batchnorm_params = BatchnormParams(-1)\n","  dropout_params = DropoutParams(0.9)\n","  final_dense_params = DenseParams(output_units, 'relu')\n","  return ModelParams(num_embedding_layers, \n","                     embedding_params, \n","                     num_dense_layers, \n","                     conv1d_params, \n","                     dense_params, \n","                     batchnorm_params, \n","                     dropout_params,\n","                     final_dense_params)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UIWrHVpZ6VQ2"},"outputs":[],"source":["from tensorflow import keras\n","\n","# support param list for Embedding, Conv1D, Dense layers.\n","class PlainDeepNet(keras.Model):\n","    def __init__(self, \n","                 string_lookups=None,\n","                 model_params=None):\n","        super().__init__()\n","         \n","        self.string_lookups = string_lookups\n","        self.model_params = model_params\n","\n","        self.embedding_layers = []\n","        embedding_params = self.model_params.embedding_params\n","        for i in range(self.model_params.num_embedding_layers):\n","          self.embedding_layers.append(keras.layers.Embedding(\n","              input_dim = embedding_params[i].input_dim + 1,\n","              output_dim = embedding_params[i].output_dim,\n","              embeddings_initializer = embedding_params[i].embeddings_initializer,\n","              embeddings_regularizer = embedding_params[i].embeddings_regularizer,\n","              activity_regularizer = embedding_params[i].activity_regularizer,\n","              embeddings_constraint = embedding_params[i].embeddings_constraint,\n","              mask_zero = embedding_params[i].mask_zero,\n","              input_length = embedding_params[i].input_length,\n","              **embedding_params[i].kwargs\n","          ))\n","          logging.info(f'constructed the {i}th emebdding layer.')\n","        \n","        self.concatenate = keras.layers.Concatenate()\n","\n","        self.conv1d_layers = []\n","        self.conv1d_normalization_layers = []\n","        conv1d_params = self.model_params.conv1d_params\n","        batchnorm_params = model_params.batchnorm_params\n","        for i in range(self.model_params.num_dense_layers):\n","            self.conv1d_layers.append(keras.layers.Conv1D(\n","                filters=conv1d_params[i].filters,\n","                kernel_size=conv1d_params[i].kernel_size,\n","                strides=conv1d_params[i].strides,\n","                padding=conv1d_params[i].padding,\n","                data_format=conv1d_params[i].data_format,\n","                dilation_rate=conv1d_params[i].dilation_rate,\n","                groups=conv1d_params[i].groups,\n","                activation=conv1d_params[i].activation,\n","                use_bias=conv1d_params[i].use_bias,\n","                kernel_initializer=conv1d_params[i].kernel_initializer,\n","                bias_initializer=conv1d_params[i].bias_initializer,\n","                kernel_regularizer=conv1d_params[i].kernel_regularizer,\n","                bias_regularizer=conv1d_params[i].bias_regularizer,\n","                activity_regularizer=conv1d_params[i].activity_regularizer,\n","                kernel_constraint=conv1d_params[i].kernel_constraint,\n","                bias_constraint=conv1d_params[i].bias_constraint,\n","                **conv1d_params[i].kwargs))\n","            self.conv1d_normalization_layers.append(\n","                keras.layers.Normalization(axis=batchnorm_params.axis))\n","\n","\n","        self.dense_layers = []\n","        self.dense_normalization_layers = []\n","        dense_params = self.model_params.dense_params\n","        for i in range(self.model_params.num_dense_layers):\n","            self.dense_layers.append(keras.layers.Dense(\n","                units=dense_params[i].units,\n","                activation=dense_params[i].activation,\n","                use_bias=dense_params[i].use_bias,\n","                kernel_initializer=dense_params[i].kernel_initializer,\n","                bias_initializer=dense_params[i].bias_initializer,\n","                kernel_regularizer=dense_params[i].kernel_regularizer,\n","                bias_regularizer=dense_params[i].bias_regularizer,\n","                activity_regularizer=dense_params[i].activity_regularizer,\n","                kernel_constraint=dense_params[i].kernel_constraint,\n","                bias_constraint=dense_params[i].bias_constraint,\n","                **dense_params[i].kwargs))\n","            self.dense_normalization_layers.append(\n","                keras.layers.Normalization(axis=batchnorm_params.axis))\n","            \n","        self.flatten = keras.layers.Flatten()\n","        '''\n","        # currently, batch normalization expects 3d data.\n","        self.normalization = keras.layers.BatchNormalization'''\n","        \n","        dropout_params = self.model_params.dropout_params\n","        self.dropout_layers = []\n","        for i in range(self.model_params.num_dense_layers):\n","            self.dropout_layers.append(\n","                keras.layers.Dropout(dropout_params.rate,\n","                                     noise_shape=dropout_params.noise_shape,\n","                                     **dropout_params.kwargs))\n","        \n","        dense_params = self.model_params.final_dense_params\n","        self.dense_final = keras.layers.Dense(units=dense_params.units,\n","                                              activation=dense_params.activation,\n","                                              use_bias=dense_params.use_bias,\n","                                              kernel_initializer=dense_params.kernel_initializer,\n","                                              bias_initializer=dense_params.bias_initializer,\n","                                              kernel_regularizer=dense_params.kernel_regularizer,\n","                                              bias_regularizer=dense_params.bias_regularizer,\n","                                              activity_regularizer=dense_params.activity_regularizer,\n","                                              kernel_constraint=dense_params.kernel_constraint,\n","                                              bias_constraint=dense_params.bias_constraint,\n","                                              **dense_params.kwargs)\n","        \n","        logging.info('construction completed.')\n","        \n","        self.softmax = tf.keras.layers.Softmax()\n","\n","    def conv1d_module(self, i, inputs):\n","        outputs = self.conv1d_layers[i](inputs)\n","        outputs = self.conv1d_normalization_layers[i](outputs)\n","        return outputs\n","    \n","    def dense_module(self, i, inputs):\n","        outputs = self.dense_layers[i](inputs)\n","        outputs = self.dense_normalization_layers[i](outputs)\n","        outputs = self.dropout_layers[i](outputs)\n","        return outputs\n","    \n","    def call(self, inputs):\n","        embeddings = []\n","        for i in range(self.model_params.num_embedding_layers):\n","            output = self.string_lookups[i](inputs[i])\n","            embeddings.append(self.embedding_layers[i](output)) \n","        concatenated_embeddings = self.concatenate(embeddings)\n","        concatenated_embeddings = tf.expand_dims(concatenated_embeddings, -1)\n","        logging.info(f'shape of concatenated embeddings: {concatenated_embeddings.shape}')\n","        outputs = concatenated_embeddings\n","        for i in range(self.model_params.num_dense_layers):\n","          outputs = self.conv1d_module(i, outputs)\n","          logging.info(f'output shape of {i}th conv1d layer: {outputs.shape}')\n","          outputs = self.flatten(outputs)\n","          outputs = self.dense_module(i, outputs)\n","          logging.info(f'output shape of {i}th dense layer: {outputs.shape}')\n","          if i != self.model_params.num_dense_layers-1:  \n","            outputs = tf.expand_dims(outputs, -1)\n","        \n","        '''\n","        for i in range(self.num_conv1d_layers):\n","            outputs = self.conv1d_module(outputs)\n","        logging.info(f'conv1d output shape: {outputs.shape}')\n","    \n","        # TODO: add the dense layer dim as a param.\n","        outputs = self.flatten(outputs)\n","        for i in range(self.num_dense_layers):\n","            outputs = self.dense_module(outputs)\n","        '''\n","        logging.info(f'shape of input to final dense layer: {outputs.shape}')\n","\n","        outputs = self.dense_final(outputs)\n","        outputs = self.softmax(outputs)\n","\n","        logging.info(f'final output shape: {outputs.shape}')\n","\n","        return outputs\n","        \n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ns-Fh_9boT-N"},"outputs":[],"source":["# Ratings data.\n","ratings = tfds.load('movielens/100k-ratings', split=\"train\")\n","# Features of all the available movies.\n","movies = tfds.load('movielens/100k-movies', split=\"train\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-B10T5rMo7CC"},"outputs":[],"source":["user_feature_keys= ['bucketized_user_age', 'raw_user_age', 'user_gender', 'user_id', 'user_occupation_label', 'user_occupation_text', 'user_zip_code']\n","movie_feature_keys = ['movie_genres', 'movie_id', 'movie_title']\n","extra_feature_keys = 'timestamp'\n","target = 'user_rating'\n","\n","user_features = ratings.map(lambda x: {\n","    'bucketized_user_age': tf.strings.as_string(x['bucketized_user_age']),\n","    'raw_user_age': tf.strings.as_string(x['raw_user_age']),\n","    'user_gender': tf.strings.as_string(x['user_gender']),\n","    'user_id': x['user_id'],\n","    'user_occupation_label': tf.strings.as_string(x['user_occupation_label']),\n","    'user_occupation_text': x['user_occupation_text'],\n","    'user_zip_code': x['user_zip_code'],\n","})\n","\n","movie_features = ratings.map(lambda x: {\n","    'movie_genres': tf.strings.as_string(tf.slice(x['movie_genres'], begin=[0], size=[1])),\n","    'movie_id': x['movie_id'],\n","    'movie_title': x['movie_title']\n","})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZizX954Ecblc"},"outputs":[],"source":["user_feature_vocabularies = defaultdict()\n","for feature in user_feature_keys:\n","  logging.info(f'processing feature {feature}.')\n","  user_feature_vocabularies[feature] = tf.keras.layers.StringLookup(mask_token=None)\n","  user_feature_vocabularies[feature].adapt(user_features.map(lambda x: x[feature] if x[feature].dtype==tf.string else tf.strings.as_string(x[feature])))\n","\n","movie_feature_vocabularies = defaultdict()\n","for feature in movie_feature_keys:\n","  movie_feature_vocabularies[feature] = tf.keras.layers.StringLookup(mask_token=None)\n","  movie_feature_vocabularies[feature].adapt(movie_features.map(lambda x: x[feature] if x[feature].dtype==tf.string else tf.strings.as_string(x[feature])))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ax8K8yO1X0d8"},"outputs":[],"source":["vocab_sizes = []\n","for vocab in user_feature_vocabularies.values():\n","  vocab_sizes.append(len(vocab.get_vocabulary()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SBqVdOnYAsUv"},"outputs":[],"source":["np.unique(list(ratings.map(lambda x: x['user_rating'])))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_YtA_LY5vEKK"},"outputs":[],"source":["# add parse example\n","\n","batched_user_features = ratings.map(lambda x: {\n","    'bucketized_user_age': tf.strings.as_string(x['bucketized_user_age']),\n","    'raw_user_age': tf.strings.as_string(x['raw_user_age']),\n","    'user_gender': tf.strings.as_string(x['user_gender']),\n","    'user_id': x['user_id'],\n","    'user_occupation_label': tf.strings.as_string(x['user_occupation_label']),\n","    'user_occupation_text': x['user_occupation_text'],\n","    'user_zip_code': x['user_zip_code'],\n","    'user_rating': x['user_rating']\n","}).prefetch(BATCH_SIZE*100).shuffle(BATCH_SIZE*50).repeat().batch(BATCH_SIZE)\n","dataset_iter = iter(batched_user_features)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ebRDE51mWxxG"},"outputs":[],"source":["num_embedding_layers=7\n","embedding_input_dims=vocab_sizes\n","embedding_output_dims=[32]*num_embedding_layers\n","output_units = OUTPUT_UNITS\n","conv1d_filters=8\n","conv1d_filter_dim=11\n","dense_shrink_multiplier=4\n","\n","model_params = constructModelParams(num_embedding_layers=7,\n","                                    embedding_input_dims=embedding_input_dims,\n","                                    embedding_output_dims=embedding_output_dims,\n","                                    output_units=output_units,\n","                                    conv1d_filters=conv1d_filters,\n","                                    conv1d_filter_dim=conv1d_filter_dim,\n","                                    dense_shrink_multiplier=dense_shrink_multiplier)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dZzB54HJHhS2"},"outputs":[],"source":["model = PlainDeepNet(\n","    string_lookups=list(user_feature_vocabularies.values()),\n","    model_params=model_params)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dKyaP-pOa0--"},"outputs":[],"source":["# TODO: adds movie_feature_vocabularies\n","lr_scheduler = tf.keras.optimizers.schedules.ExponentialDecay(\n","    0.3, 500, 0.9, staircase=False, name=None\n",")\n","optimizer = tf.keras.optimizers.Adam(learning_rate=lr_scheduler)\n","loss_func = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","metric = tf.keras.metrics.Accuracy()\n","\n","def get_loss(y_pred, y, output_units=1):\n","  return tf.keras.losses.MeanSquaredError()(y_pred, y)\n","\n","# y_one_hot = tf.one_hot(tf.cast(tf.math.subtract(inputs[-1], 1), dtype=tf.int32), OUTPUT_UNITS, dtype=tf.int32)\n","# y_one_hot.set_shape([BATCH_SIZE, OUTPUT_UNITS])  \n","\n","input_signature = [[\n","    tf.TensorSpec(shape=(BATCH_SIZE, ), dtype=tf.string, name=None), \n","    tf.TensorSpec(shape=(BATCH_SIZE, ), dtype=tf.string, name=None), \n","    tf.TensorSpec(shape=(BATCH_SIZE, ), dtype=tf.string, name=None), \n","    tf.TensorSpec(shape=(BATCH_SIZE, ), dtype=tf.string, name=None), \n","    tf.TensorSpec(shape=(BATCH_SIZE, ), dtype=tf.string, name=None), \n","    tf.TensorSpec(shape=(BATCH_SIZE, ), dtype=tf.string, name=None), \n","    tf.TensorSpec(shape=(BATCH_SIZE, ), dtype=tf.string, name=None), \n","    tf.TensorSpec(shape=(BATCH_SIZE, ), dtype=tf.float32, name=None)\n","]]\n","@tf.function(input_signature=input_signature)\n","def train_step(inputs):\n","  with tf.GradientTape() as tape:\n","    outputs = model(inputs[:-1])  \n","    targets = tf.subtract(inputs[-1], 1)\n","    loss = loss_func(targets, outputs)\n","    predicts = tf.math.argmax(outputs, -1)\n","    metric.update_state(targets, predicts)\n","    # loss = get_loss(outputs, inputs[-1], output_units=OUTPUT_UNITS)\n","    # print(model.trainable_variables)\n","    gradients = tape.gradient(loss, model.trainable_variables)\n","    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","  result = {'loss': loss, 'metric': metric.result()}\n","  # result = {'loss': loss, 'metric': metric.result(), 'targets': targets, 'predicts': predicts, 'outputs': outputs}\n","  return result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iyPRQ2H1vyUM"},"outputs":[],"source":["# TODO: adds movie_feature_vocabularies\n","for i in range(20000):\n","  batch = dataset_iter.get_next()\n","  inputs = list(batch.values())\n","  # print(inputs[0].shape)\n","  result = train_step(inputs)\n","  loss = result['loss'].numpy()\n","  metric = result['metric'].numpy()\n","  if i%100 == 0:\n","    logging.info(f'step {i} loss: {loss}')\n","    logging.info(f'metric: {metric}')"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNkHbihcS1G+Ctz6lspDQsq","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.16"}},"nbformat":4,"nbformat_minor":0}
